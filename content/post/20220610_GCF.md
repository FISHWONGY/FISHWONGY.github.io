---
authors:
- Hugo Authors
date: "2022-06-10"
excerpt: Working with Google Cloud Functions
hero: /images/on_gcf_20220610/gcf_logo.png
title: On Google Cloud Functions
---

<div style="text-align: justify">

Google Cloud Functions is a highly scalable serverless FaaS product. Previously, I have used it for some data extraction and cleaning tasks at work. Recently, I have decided to incorporate it into one of my personal projects.

The workflow looks something like below ðŸ‘€

<p align="center">
<img alt = 'png' src='/images/on_gcf_20220610/gcf_workflow.png'/>
</p>









<u><b>
    <p style="font-size:20pt;">
      Cloud Function Setup
    </p>
</b></u>

So, let's look into the setup and begin to write some code.

When setting up the cloud function, you will need to have a runtime service account, and depending on development needs, we can also adjust the memory and timeout limit. Optionally, we can set different environment variables and secrets.


<p align="center">
<img alt = 'png' width='800' src='/images/on_gcf_20220610/gcf_setup1.png'/>
</p>

For code structure, we can have something like this,

```md
â”œâ”€â”€ __init__.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ utils.py
â”œâ”€â”€ functions.py
â””â”€â”€ main.py
```

Note that on the cloud function, the entry point has to be the final function that runs in the `main.py`. And in that function, a parameter request has to be passed in order for the cloud function's HTTPS to be triggered.

Also, if the function is returning a string at the end, it can't be anything other than `2XX`. I once saw a use case where a cloud function was used for some pipeline QA, and the function is programmed in a way that if the pipelines' QA checks didn't pass, it returns a `non-2XX` code. And because of this, the cloud function has been failing until that logic is removed.


<p align="center">
<img alt = 'png' width='800' src='/images/on_gcf_20220610/gcf_setup2.png'/>
</p>


<u><b>
    <p style="font-size:20pt;">
      Cloud Scheduler Setup
    </p>
</b></u>

After deploying the cloud function successfully, the next step is to schedule it using GCP's Scheduler. From our cloud function, we can obtain a `Trigger URL` under the Trigger Tab, which we can use for setting up our scheduler's trigger.

Let's take a look at the scheduler's setup.

Here we can see we have to set a frequency in [cron syntax](https://crontab.guru/) for the cronjob; here, I will have the job run hourly.

<p align="center">
<img alt = 'png' src='/images/on_gcf_20220610/gcf_setup3.png'/>
</p>


The `URL` here is the `trigger url` that we obtain from the function.
Please note we will need a service account here with sufficient trigger priviledge for the job.
<p align="center">
<img alt = 'png' src='/images/on_gcf_20220610/gcf_setup4.png'/>
</p>


And that's it! Every hour, the Cloud Function will be triggered. If there is news that hasn't been published yet, it will be published to the Discord channel!


<p align="center">
<img alt = 'png' src='/images/on_gcf_20220610/gcf_dc_news.png'/>
</p>


---
Some final thoughts...



1. Scheduler is just one of the methods to trigger a Cloud Function. Apart from that, we can also use Pub/Sub to trigger a function. For example, when a new file lands in a GCP bucket, a Cloud Function will be triggered to run.


2. When connecting Cloud Function with BigQuery, I've learned that the region of the bucket has to be the same as the region of the dataset in BQ. If not, data can't be loaded successfully.

That's it for now. 

Thank you for reading and have a nice day!

